Perfect üëç ‚Äî let‚Äôs go **step-by-step** from **zero setup** ‚Äî
this will help you install **Hadoop** on a **fresh Ubuntu system** (no SSH, no Java, nothing yet).

We‚Äôll set up everything: **Java ‚Üí SSH ‚Üí Hadoop** and test it works.
‚úÖ Works for Ubuntu 20.04, 22.04, 24.04.
‚úÖ No previous Hadoop setup required.

---

## üß≠ OVERVIEW

You‚Äôll do 7 main steps:

1. Install Java
2. Add Hadoop user
3. Set up SSH (for Hadoop to talk to itself)
4. Download & install Hadoop
5. Configure environment variables
6. Configure Hadoop core files
7. Format HDFS & start Hadoop

---

## ü™ú STEP 1 ‚Äî Install Java

Hadoop needs Java.

```bash
sudo apt update
sudo apt install openjdk-11-jdk -y
```

‚úÖ Verify installation:

```bash
java -version
```

You should see something like:

```
openjdk version "11.0.xx" ...
```

---

## üßë‚Äçüíª STEP 2 ‚Äî Create Hadoop User

It‚Äôs best not to run Hadoop as root.

```bash
sudo adduser hadoop
```

(it‚Äôll ask for a password ‚Äî you can use something simple like `hadoop`)

Then add permissions:

```bash
sudo usermod -aG sudo hadoop
```

Switch to that user:

```bash
su - hadoop
```

---

## üîê STEP 3 ‚Äî Set up SSH (for Hadoop communication)

Hadoop uses SSH to manage nodes (even your own machine).

Install SSH:

```bash
sudo apt install ssh -y
```

Generate SSH key (press Enter for all prompts):

```bash
ssh-keygen -t rsa -P ""
```

Add your key to authorized keys:

```bash
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```

Now test SSH:

```bash
ssh localhost
```

The first time it will ask:

```
Are you sure you want to continue connecting (yes/no)?
```

Type `yes`.

If it logs you in automatically ‚Äî SSH is working ‚úÖ
(Exit with `exit` if you get a new shell.)

---

## üì¶ STEP 4 ‚Äî Download & Install Hadoop

Go to the Apache Hadoop downloads page (or run the command):

```bash
wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
```

Extract and move it:

```bash
tar -xvzf hadoop-3.3.6.tar.gz
sudo mv hadoop-3.3.6 /usr/local/hadoop
```

---

## ‚öôÔ∏è STEP 5 ‚Äî Configure Environment Variables

Open the bash file for the `hadoop` user:

```bash
nano ~/.bashrc
```

Scroll to the bottom and add:

```bash
# Hadoop Environment Variables
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

Save and apply:

```bash
source ~/.bashrc
```

‚úÖ Test:

```bash
hadoop version
```

If it shows Hadoop version ‚Äî success üéâ

---

## üßæ STEP 6 ‚Äî Configure Hadoop Files

You‚Äôll configure 5 files (all inside `/usr/local/hadoop/etc/hadoop`).

### 1Ô∏è‚É£ `hadoop-env.sh`

```bash
nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
```

Find this line and set:

```bash
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

---

### 2Ô∏è‚É£ `core-site.xml`

```bash
nano $HADOOP_HOME/etc/hadoop/core-site.xml
```

Add inside `<configuration>`:

```xml
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000</value>
  </property>
</configuration>
```

---

### 3Ô∏è‚É£ `hdfs-site.xml`

```bash
nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
```

Add:

```xml
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <property>
    <name>dfs.name.dir</name>
    <value>file:/usr/local/hadoop/hadoop_data/hdfs/namenode</value>
  </property>
  <property>
    <name>dfs.data.dir</name>
    <value>file:/usr/local/hadoop/hadoop_data/hdfs/datanode</value>
  </property>
</configuration>
```

---

### 4Ô∏è‚É£ `mapred-site.xml`

Create it from the template:

```bash
cp $HADOOP_HOME/etc/hadoop/mapred-site.xml.template $HADOOP_HOME/etc/hadoop/mapred-site.xml
nano $HADOOP_HOME/etc/hadoop/mapred-site.xml
```

Add:

```xml
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```

---

### 5Ô∏è‚É£ `yarn-site.xml`

```bash
nano $HADOOP_HOME/etc/hadoop/yarn-site.xml
```

Add:

```xml
<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
</configuration>
```

---

## üíæ STEP 7 ‚Äî Format HDFS & Start Hadoop

Create directories:

```bash
sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/namenode
sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode
sudo chown -R hadoop:hadoop /usr/local/hadoop
```

Format the HDFS:

```bash
hdfs namenode -format
```

Start Hadoop daemons:

```bash
start-dfs.sh
start-yarn.sh
```

‚úÖ Check they are running:

```bash
jps
```

You should see:

```
NameNode
DataNode
ResourceManager
NodeManager
SecondaryNameNode
```

---

## üåê STEP 8 ‚Äî Web Interfaces

| Component       | URL                                            |
| --------------- | ---------------------------------------------- |
| NameNode        | [http://localhost:9870](http://localhost:9870) |
| DataNode        | [http://localhost:9864](http://localhost:9864) |
| ResourceManager | [http://localhost:8088](http://localhost:8088) |

---

## üßπ Stop Hadoop (when done)

```bash
stop-yarn.sh
stop-dfs.sh
```

---

### ‚úÖ You‚Äôve successfully installed Hadoop on Ubuntu!

---

Would you like me to give you a **single script (.sh)** that automatically runs all these steps for you (so you can just copy-paste and run it)?
